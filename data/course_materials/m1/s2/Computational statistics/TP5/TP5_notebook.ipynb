{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth Lab Session: Statistical uncertainty'\n",
    "UE Computational Statistics'\n",
    "\n",
    "\n",
    "# Part 0. Installations\n",
    "\n",
    "We will need the following packages:\n",
    "\n",
    "- JAX: a numerical computing library for Python that is composable, fast, and differentiable\n",
    "- PyMC: a python library for probabilistic programming\n",
    "- Arviz: a python library for exploratory analysis of Bayesian models\n",
    "\n",
    "In an Anaconda Terminal/Console/Command Prompt, run the following:\n",
    "\n",
    "1. Create a new environment with `conda create -c conda-forge -n comp_stat \"pymc>=5\"`\n",
    "2. Activate the environment with `conda activate comp_stat`\n",
    "3. Install the packages with `conda install -c conda-forge jax jaxlib arviz ipykernel ipywidgets python-graphviz`\n",
    "4. Check that our favorite packages are installed with `conda install numpy scipy matplotlib seaborn pandas`\n",
    "5. (Optionnal) If you want PyMC to be efficient, you can also install numpyro with `conda install -c conda-forge numpyro`\n",
    "6. Finally, in the Jupyter Notebook, be sure to select the kernel `comp_stat` to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, hessian, jit\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Logistic regression\n",
    "\n",
    "## 1.1 The statistical model\n",
    "\n",
    "The statistical model of the logistic regression is as follows. First, the random datset is:\n",
    "$$\n",
    "D = \\begin{pmatrix} \\mathbf Y & \\mathbf X \\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "Y_1 & X_{11} & \\cdots & X_{1p}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "Y_n & X_{n1} & \\cdots & X_{np}\n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "where $Y_i \\in \\{0,1\\}$ is the binary response variable, and $X_{ij}$ are the $j$-th covariate of the $i$-th observation. The rows of $D$ are iid, the marginal distribution of $X_i=(X_{i1},\\ldots,X_{ip})$ is not specified, and the conditional distribution of $Y_i$ given $(X_{i1},\\ldots,X_{ip})$ is a Bernoulli distribution with parameter $p(X_i)$:\n",
    "$$\n",
    "[Y_i|X_i] \\sim \\text{Bernoulli}\\Big(p(X_i)\\Big), \\quad \\text{with }\n",
    "p(X_i)= \\frac{1}{1+\\exp\\left(-\\beta_0-\\sum_j\\beta_j X_{ij}\\right)}.\n",
    "$$\n",
    "\n",
    "The (conditional) likelihood of the data $d$ is:\n",
    "$$\n",
    "f(\\mathbf y|\\beta, \\mathbf x) = \\prod_{i=1}^n p(x_i)^{y_i} \\left(1-p(x_i)\\right)^{1-y_i}.\n",
    "$$\n",
    "\n",
    "## 1.2 The data\n",
    "\n",
    "The dataset is available in the `wells.csv` file. We can import it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>switch</th>\n",
       "      <th>arsenic</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist100</th>\n",
       "      <th>assoc</th>\n",
       "      <th>educ</th>\n",
       "      <th>educ4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16.826000</td>\n",
       "      <td>0.16826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.71</td>\n",
       "      <td>47.321999</td>\n",
       "      <td>0.47322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.07</td>\n",
       "      <td>20.966999</td>\n",
       "      <td>0.20967</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.15</td>\n",
       "      <td>21.486000</td>\n",
       "      <td>0.21486</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.10</td>\n",
       "      <td>40.874001</td>\n",
       "      <td>0.40874</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   switch  arsenic       dist  dist100  assoc  educ  educ4\n",
       "0       1     2.36  16.826000  0.16826      0     0    0.0\n",
       "1       1     0.71  47.321999  0.47322      0     0    0.0\n",
       "2       0     2.07  20.966999  0.20967      0    10    2.5\n",
       "3       1     1.15  21.486000  0.21486      0    12    3.0\n",
       "4       1     1.10  40.874001  0.40874      1    14    3.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('wells.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is taken from the book by Gelman, Hill, and Vehtari (2020). The wells used by the inhabitants of Bangladesh are contaminated with natural arsenic, as in other South Asian countries. Arsenic is a poison whose risks accumulate proportionally to the duration and dose of exposure. A well is considered safe when the arsenic dose it contains is less than $0.5$ (in hundreds of micrograms per liter). The wells are located in living areas. When a well is not safe, it is very common for a user to find a safe well in their neighborhood without over-exploiting and drying it up because the potable water needed for human consumption represents a small volume.\n",
    "\n",
    "The data studied comes from the work of a research team from the USA and Bangladesh on wells in the Araihazar region. This team measured the arsenic level of all wells in the region and labeled them as \"safe\" or \"unsafe\". Households that were using unsafe wells were encouraged to switch wells. A few years later, the research team returned to the field to find out which households had (or had not) switched wells. The observations in the dataset correspond to different households in this region. The variables are:\n",
    "\n",
    "-   `switch`: 1 if the household has switched wells, 0 otherwise\n",
    "-   `arsenic`: the level of arsenic (in hundreds of micrograms per liter)\n",
    "-   `assoc`: 1 if a household member is active in community organizations, 0 otherwise\n",
    "-   `dist`: the distance in meters to the nearest safe well\n",
    "-   `educ`: education level of the household head\n",
    "\n",
    "Note that two other variables are in the dataset: `educ4` which is mainly `educ/4` and `dist100` which is mainly `dist/100`.\n",
    "\n",
    "The response variable is `switch`. To set the covariates, we standardize the variables `arsenic`, `assoc`, `dist`, and `educ` to have mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3020, 4), (3020,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = jnp.array(data['switch'])\n",
    "x = jnp.array(data[['arsenic', 'assoc', 'dist', 'educ']])\n",
    "x = (x - x.mean(axis=0)) / x.std(axis=0)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we add a column of ones to the covariate matrix to account for the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3020, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = jnp.hstack((jnp.ones((x.shape[0], 1)), x))\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first study of the data has been done with `statsmodels`. We want to recover these results in the following sections. The output of the analysis was:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Dep. Variable: | y   | No. Observations: | 3020 |\n",
    "| --- | --- | --- | --- |\n",
    "| Model: | GLM | Df Residuals: | 3015 |\n",
    "| Model Family: | Binomial | Df Model: | 4   |\n",
    "| Link Function: | Logit | Scale: | 1.0000 |\n",
    "| Method: | IRLS | Log-Likelihood: | \\-1953.9 |\n",
    "| Date: | Wed, 05 Mar 2025 | Deviance: | 3907.8 |\n",
    "| Time: | 10:22:22 | Pearson chi2: | 3.05e+03 |\n",
    "| No. Iterations: | 4   | Pseudo R-squ. (CS): | 0.06726 |\n",
    "| Covariance Type: | nonrobust |     |     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|     | coef | std err | z   | P>abs(z) | \\[0.025 | 0.975\\] |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| const | 0.3364 | 0.038 | 8.740 | 0.000 | 0.261 | 0.412 |\n",
    "| x1  | 0.5171 | 0.046 | 11.226 | 0.000 | 0.427 | 0.607 |\n",
    "| x2  | \\-0.0614 | 0.038 | \\-1.615 | 0.106 | \\-0.136 | 0.013 |\n",
    "| x3  | \\-0.3448 | 0.040 | \\-8.569 | 0.000 | \\-0.424 | \\-0.266 |\n",
    "| x4  | 0.1705 | 0.039 | 4.427 | 0.000 | 0.095 | 0.246 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3 Fisher information\n",
    "\n",
    "Write the log-likelihood function of the logistic regression model, using `jnp` functions. The input should be:\n",
    "\n",
    "- `beta`: the parameter vector of the logistic regression model,\n",
    "- `x`: the covariate matrix (including a first column of ones for the intercept),\n",
    "- `y`: the response vector.\n",
    "\n",
    "To this aim, you can use `jnp.dot` for matrix product, `jnp.exp`, `jnp.log`, and `jnp.sum`. No loop (explicit or implicit) is needed. To accelerate the computation, you can use the `@jit` decorator before the function definition.\n",
    "\n",
    "The `@jit` decorator is used to ask compilation of the function by JAX (JIT stands for **Just-In-Time compilation**). This is a way to speed up the computation. The first time the function is called, JAX compiles the function and stores the compiled version. The next time the function is called, JAX uses the compiled version. The compilation time is not negligible, but the computation time is much faster. The compilation time is negligible when the function is called many times, which will be the case of the log-likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **automatic differentiation** in JAX, define an object that computes the gradient of the log-likelihood function with respect to the parameter vector `beta`, that is to say the score function. Do the same for the Hessian matrix. The input should be the same as for the log-likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: In JAX, to use automatic differentiation, you need a pure function. That is why we need to pass `x1` and `y` as arguments of the log-likelihood function.\n",
    "\n",
    "Write a python function that takes as input:\n",
    "\n",
    "- `init`: the initial value of the parameter vector,\n",
    "- `n_iter`: the number of iterations of the gradient descent algorithm,\n",
    "- `step_size`: the factor in front of the step size that is proportional to $1/\\sqrt{t+1}$,\n",
    "- `x`: the covariate matrix (including a first column of ones for the intercept),\n",
    "- `y`: the response vector,\n",
    "\n",
    "and returns the maximum likelihood estimator `beta_MLE` of the logistic regression model. The function implements a gradient descent algorithm. (Warning: we want to **maximize** the log-likelihood function, not minimize it. Be sure to climb the hill, not to go down the hill.)\n",
    "\n",
    "We want to accelerate the computations wit JIT compilation. Yet we have to tell JIT that the `n_iter` argument is static, that is to say that it does not change along the iterations. Otherwise we will not be able to use it to control the loop. To this aim, we use the `partial` function from the `functools` package. For example, if your inputs are in the same order as above, the decorator should be: `@partial(jit, static_argnums=(1,))`. To use this decorator, you have to import it with `from functools import partial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm on `x1` and `y` defined above. The initial value of the parameter vector is `jnp.zeros(x.shape[1])`. The step size should be `0.005/np.sqrt(t+1)` where `t` is the iteration number. The algorithm stops after 200 iterations. Compare your results with the one from `statsmodels` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the observed information matrix at the MLE and use it to get the standard errors of each coordinate of the MLE. Compare your results with the one from `statsmodels` above. You can use `jnp.diag` and `jnp.linalg.inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results with the one from `statsmodels` above.\n",
    "\n",
    "**Final remark:** to be sure that the output of the MLE function is the global maximum of the log-likelihood function, you can run the algorithm several times with different initial values of the parameter vector. At the end, we keep the best result.\n",
    "\n",
    "## 1.4 The Bootstrap\n",
    "\n",
    "Write a python function that takes as input:\n",
    "\n",
    "- `x`: the covariate matrix (including a first column of ones for the intercept),\n",
    "- `y`: the response vector,\n",
    "- `B`: the number of bootstrap samples,\n",
    "\n",
    "and return $B$ bootstrap estimates of $\\beta$ for the logistic regression model. To sample individuals with replacement, i.e. numbers between 0 and $n-1$, you can use `np.random.choice(n, n, replace=True)`. We will not use JAX nor JIT for this function. There is two good reasons for that: (1) using RNG in JAX is difficult, (2) we need to collect the $B$ estimates in a loop, which is difficult with JAX/JIT. See next section for a JAX implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm on `x1` and `y` defined above with `B=1000` and compute the standard deviation of each coordinate of the bootstrap estimates. Compare your results with the one from the observed information matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms of the bootstrap estimates of each coordinate of $\\beta$. Add the bell curve of the normal distribution with mean the MLE and standard deviation the standard errors of the MLE to each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Using RNG with JAX\n",
    "\n",
    "Using RNG in JAX is not straightforward. The main difficulty is that the state of the RNG is not a hidden variable that is automatically updated when you use the RNG. In JAX, the state of the RNG is named a key. We have to explicitly:\n",
    "\n",
    "- create a RNG key,\n",
    "- pass it as an argument to the function that uses the RNG,\n",
    "- update the key each time it is useful.\n",
    "\n",
    "Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4983746  -1.4730613 ]\n",
      " [-0.23836732  2.0859866 ]\n",
      " [-0.2981817  -0.32104772]\n",
      " [-0.0488972  -0.24350879]\n",
      " [ 1.846001    0.25888392]\n",
      " [ 0.38826737  0.183464  ]\n",
      " [-0.9135562   0.27851298]\n",
      " [-1.1528534  -0.9781027 ]\n",
      " [-0.67853504 -0.7576526 ]\n",
      " [ 0.618905    0.7398856 ]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "key = jax.random.key(1234) # create a RNG key with seed=1234\n",
    "x=[]\n",
    "for i in range(10):\n",
    "    # update the key, create a subkey to use the RNG\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # then use it! \n",
    "    x.append(jax.random.normal(subkey, (1, 2)))\n",
    "xnp = jnp.vstack(x)\n",
    "print(xnp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also have created the RNG keys outside the loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.95239824  0.34621808]\n",
      " [ 1.4983746  -1.4730613 ]\n",
      " [ 0.32401362  1.3939046 ]\n",
      " [ 2.0800176  -0.03578897]\n",
      " [-0.9054026   0.9976389 ]\n",
      " [ 0.7658792  -0.58717984]\n",
      " [ 1.3204138   0.5375884 ]\n",
      " [ 2.2415018   1.2188705 ]\n",
      " [ 0.13732556  0.42808887]\n",
      " [ 0.23492298 -1.3205155 ]]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(1234) # create a RNG key with seed=1234\n",
    "keys = jax.random.split(key, 10) # create an array of 10 subkeys\n",
    "x=[]\n",
    "for i in range(10):\n",
    "    x.append(jax.random.normal(keys[i], (1, 2)))\n",
    "xnp = jnp.vstack(x)\n",
    "print(xnp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 The JAX Bootstrap\n",
    "\n",
    "Now that we have understood (a bit) how to use the RNG in JAX, we are in a better position to implement the bootstrap with JAX. Yet we need to deal with the loop part of the algorithm. The idea is to implement the body of the loop in a specific function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def one_bootstrap(rng_key, x, y):\n",
    "    n = x.shape[0]\n",
    "    idx = jax.random.choice(rng_key, n, (n,), replace=True)\n",
    "    return MLE(jnp.zeros(x1.shape[1]), 200, 0.005, x[idx], y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, all we need is to **vectorize** the function `one_bootstrap` with the `vmap` function of JAX. The idea is that the new function will take an array of `rng_key`'s and apply the function `one_bootstrap` to each element of the array. This will create the loop. \n",
    "\n",
    "Note that we want `x` and `y` to be constant values. The `in_axes` argument of `vmap` is used to specify over which argument we iterate in the loop. Here, we want to iterate over rng_key, but not on `x` and `y` and thus `in_axes` will be `(0,None,None)`. The `out_axes` argument is used to specify the output of the function. Use `out_axes=0` to get a 1D array of the results. The code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_jax = jax.vmap(one_bootstrap, in_axes=(0, None, None), out_axes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it, we need to create the RNG keys and run the algorithm, and then create the `jnp.array` of the results. The code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20128/108312499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# key = jax.random.key(1234) # has already been done above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbeta_boot_jax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbootstrap_jax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbeta_boot_jax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_boot_jax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'B' is not defined"
     ]
    }
   ],
   "source": [
    "# key = jax.random.key(1234) # has already been done above\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, B)\n",
    "beta_boot_jax = bootstrap_jax(keys, x1, y)\n",
    "beta_boot_jax = jnp.vstack(beta_boot_jax)\n",
    "se_boot_jax = jnp.std(beta_boot_jax, axis=0)\n",
    "print(round(se_boot_jax, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to implement the loop in JAX is to use the `fori_loop` function in the `lax` module. The `fori_loop` function is a bit like the `reduce` function in Python. It iterates a function over a range of integers. The `state` variable contains all variables used and modified in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body(i, state):\n",
    "    key, beta_boot = state\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # JAX version of beta_boot[i] = MLE(...):\n",
    "    beta_boot = beta_boot.at[i].set(MLE(jnp.zeros(x1.shape[1]), 200, 0.005, x1, y)) \n",
    "    return key, beta_boot\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "state = (subkey, jnp.zeros((B, x1.shape[1])))\n",
    "beta_boot_jax_v2 = jax.lax.fori_loop(0, B, body, state)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have use `x = x.at[i].set(value)` instead of `x[i]=value` to update the array `beta_boot` in the `body` function since the latter is not allowed in JAX.\n",
    "\n",
    "# Part 2. The Bayesian logistic regression\n",
    "\n",
    "At $\\beta=(0,\\ldots,0)$ the logistic model says that, whatever $X_i$, $[Y_i|X_i]\\sim \\text{Bernoulli}(0.5)$. Moreover, covariates have been standardized: they are comparable in the sense that they are in the same unit. Even the column that is constant, equal to $1$ can be considered as a covariate of the same unit. Hence, without looking at the actual data, it is reasonable to assume that the prior distribution of $\\beta$ is centered at $0$ and has variance $2\\times I_5$, where $I_5$ is the identity matrix:\n",
    "$$\n",
    "\\beta \\sim \\mathscr N(0, 2I_5).\n",
    "$$ \n",
    "This prior is a brake to large values of $\\beta$ that may appear when the parameters are overfitted to the data. \n",
    "\n",
    "Now, we want to perform a Bayesian analysis of the data.\n",
    "\n",
    "## 2.1 With PyMC\n",
    "\n",
    "`PyMC` is a bit as if we replace the pain of probability calculus with the pain of installing and using a complex Python package. But it is worth it. Please, install it properly following the instructions at the beginning of the notebook.\n",
    "\n",
    "The following lines of code define the Bayesian logistic regression model with PyMC. The prior distribution of $\\beta$ is a normal distribution with mean $0$ and standard deviation $2$ for each coordinate. The likelihood is a Bernoulli distribution with parameter $p(X_i)$ as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "x1np = np.array(x1)\n",
    "ynp = np.array(y)\n",
    "with pm.Model() as logistic_model:\n",
    "    X = pm.Data('X', x1np)\n",
    "    y_train = pm.Data('y', ynp)\n",
    "    # prior\n",
    "    beta = pm.Normal('beta', mu=0, sigma=np.sqrt(2), shape=5)\n",
    "    # likelihood\n",
    "    mu = pm.math.dot(X, beta)\n",
    "    p = pm.Deterministic('p', pm.math.invlogit(mu))\n",
    "    yobs = pm.Bernoulli('y_obs', p=p, observed=y_train)\n",
    "pm.model_to_graphviz(logistic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the DAG of the Bayesian model, a graphical representation of the dependencies between the variables. Nodes are in gray when observed, and in white when unobserved. The arrows indicate the dependencies between the variables that has been described in the code. The dimensions of the variables are indicated in the nodes. Check them to be sure that the model is correctly defined.\n",
    "\n",
    "Next, we can run the sample to get a sample from the posterior distribution of $\\beta$. We use 4 chains of 1000 iterations each, with 1000 tuning iterations first. The `cores` argument is set to 1 to avoid a bug with the `pymc` package and to use only one core of the CPU. In the `trace` object, we expect thus $4\\times 1000 = 4000$ draws from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model:\n",
    "    trace = pm.sample(1000, tune=1000, model=logistic_model, cores = 1, \n",
    "    chains = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `az.summary` function to get a summary of the posterior distribution of $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace, var_names=[\"beta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=\"beta\", compact=False)\n",
    "az.plot_posterior(trace, var_names=\"beta\", kind='kde')\n",
    "az.plot_forest(trace, var_names=\"beta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first series of plots shows that the four chains are sampled the same part of the parameter space. This is the expected result.\n",
    "- The second series of plots shows the posterior distribution of each coordinate of $\\beta$, with the kernel density estimate, and the $94\\%$ highest posterior density interval, which are credible intervals of probability $94\\%$, and the mean of the posterior distribution.\n",
    "- The final plot compares the credible intervals of probability $94\\%$ we get from the 4 chains. They should be equal, up to a Monte Carlo noise.\n",
    "\n",
    "\n",
    "## 2.2 With a Metropolis-Hastings algorithm\n",
    "\n",
    "We can also implement a Metropolis-Hastings algorithm to sample from the posterior distribution of $\\beta$. \n",
    "\n",
    "The first ingredient is a function that computes the log-posterior of $\\beta$. Based on the log-likelihood function defined above, implement a log_posterior function that takes as input:\n",
    "\n",
    "- `beta`: the parameter vector of the logistic regression model,\n",
    "- `x`: the covariate matrix (including a first column of ones for the intercept),\n",
    "- `y`: the response vector\n",
    "\n",
    "and returns the log-posterior of $\\beta$, using JAX and the @jit decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second ingredient is the proposal kernel. In this simple situation we can propose a new value $\\zeta$ from $\\mathscr N(\\beta, \\Sigma)$. This is a symmetric kernel, so the log-acceptance ratio is the difference between the log-posterior at $\\zeta$ and the log-posterior at $\\beta$. Implement the Metropolis-Hastings algorithm as a function with the following input:\n",
    "\n",
    "- `x`: the covariate matrix (including a first column of ones for the intercept),\n",
    "- `y`: the response vector,\n",
    "- `n_iter`: the number of iterations of the algorithm,\n",
    "- `beta0`: the initial value of the parameter vector,\n",
    "- `Sigma`: the covariance matrix of the proposal kernel.\n",
    "\n",
    "The function should return the sample of the parameter vector and the acceptance rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm on `x1` and `y` defined above with `n_iter=4500`, and `Sigma=0.002*np.eye(x1.shape[1])`, starting from the MLE, . Discard the first $500$ draws as a burn-in period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results with the one from PyMC above.\n",
    "\n",
    "Draw the five paths of the Metropolis-Hastings algorithm for each coordinate of $\\beta$, starting at $t=0$, as well as the resulting distribution of the last $4000$ draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a second chain by running the algorithm with the same parameters, but starting from $\\beta=(0,\\ldots,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 A Metropolis-Hastings algorithm with JAX\n",
    "\n",
    "There is two major difficulties with JAX:\n",
    "\n",
    "- using a random number generator (RNG) in a JAX function is not straightforward; (we have seen this problem in the previous section),\n",
    "- iterative algorithms (where the state at time $t+1$ depends on the state at time $t$) are not straightforward either.\n",
    "\n",
    "The second difficulty is the loop over the iterations. The main ingredient is to program on iteration of the Metropolis-Hastings algorithm as a function that takes as input:\n",
    "\n",
    "- `rng_key`: the RNG key,\n",
    "- `logpdf`: a function that computes the log-posterior of $\\beta$,\n",
    "- `logpdf_args`: the arguments of the log-posterior function,\n",
    "- `position`: the current state,\n",
    "- `log_prob`: the current log-probability of the current state.\n",
    "\n",
    "The ouput should be the new state and the new log-probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def rw_metropolis_kernel(rng_key, logpdf, logpdf_args, position, log_prob):\n",
    "    key, subkey = jax.random.split(rng_key)\n",
    "    move = jax.random.normal(subkey, position.shape) * 0.045\n",
    "    proposal = position + move\n",
    "    log_prob_proposal = logpdf(proposal, *logpdf_args)\n",
    "    log_uniform = jnp.log(jax.random.uniform(subkey))\n",
    "    do_accept = log_uniform < log_prob_proposal - log_prob\n",
    "    position = jnp.where(do_accept, proposal, position)\n",
    "    log_prob = jnp.where(do_accept, log_prob_proposal, log_prob)\n",
    "    return position, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can run the Metropolis-Hastings algorithm as follows. Along time, the position is stored into the `chain` object. The for-loop is replaced by a `jax.lax.fori_loop` function. The function that is iterated must have two inputs: the loop index `i` and the state of the loop, and must return the new state of the loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(1,2))\n",
    "def rw_metropolis_sampler(rng_key, n_draws, logpdf, logpdf_args, initial_position):\n",
    "    def mh_update(i, state):\n",
    "        key, position, log_prob, chain = state\n",
    "        _, key = jax.random.split(key)\n",
    "        new_position, new_log_prob = rw_metropolis_kernel(key, logpdf, logpdf_args, position, log_prob)\n",
    "        chain = chain.at[i].set(new_position)\n",
    "        return (key, new_position, new_log_prob, chain)\n",
    "    logp = logpdf(initial_position, *logpdf_args)\n",
    "    chain = jnp.zeros((n_draws, initial_position.shape[0]))\n",
    "    rng_key, position, log_prob, chain = jax.lax.fori_loop(0, n_draws, mh_update, (rng_key, initial_position, logp, chain))\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to run 4 chains in parallel, we use the `vmap` function. We have to loop over the chains: the `in_axes` argument of `vmap` is used to specify over which argument we iterate in the loop. The `out_axes` argument is used to specify how the results are collected.\n",
    "\n",
    "- The number of draws or iterations, the log-posterior function, and its arguments are the same for all chains. Thus two `0` are set in the `in_axes` argument, according to the position of these arguments in the function signature.\n",
    "- The RNG key, the initial position, and the output chain are different for each chain. Thus, three `None`'s are set in the `in_axes` argument, according to the position of these arguments in the function signature. \n",
    "\n",
    "The output chain is a 3D array, with the first dimension corresponding to the chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = 4500\n",
    "n_chains = 4\n",
    "key = jax.random.key(1234)\n",
    "run_keys = jax.random.split(key, n_chains)\n",
    "initial_positions = jnp.zeros((n_chains, x1.shape[1]))\n",
    "run_mcmc = jax.vmap(rw_metropolis_sampler, in_axes=(0, None, None, None, 0), out_axes=0)\n",
    "positions = run_mcmc(run_keys, n_draws, log_likelihood, (x1, y), initial_positions)\n",
    "positions.block_until_ready()\n",
    "print(positions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output can be easily transformed into an arviz InferenceData object, and the results can be studied with the `az` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace2 = az.convert_to_inference_data({\"beta\": positions[:, 500:, :]})\n",
    "print(az.summary(trace2, var_names=[\"beta\"]))\n",
    "az.plot_trace(trace2, var_names=\"beta\", compact=False)\n",
    "az.plot_posterior(trace2, var_names=\"beta\", kind='kde')\n",
    "az.plot_forest(trace2, var_names=\"beta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! If your computer is fast enough, you can increase the number of draws to get a better approximation of the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = 40500\n",
    "n_chains = 4\n",
    "key = jax.random.key(12345)\n",
    "run_keys = jax.random.split(key, n_chains)\n",
    "initial_positions = jnp.zeros((n_chains, x1.shape[1]))\n",
    "run_mcmc = jax.vmap(rw_metropolis_sampler, in_axes=(0, None, None, None, 0), out_axes=0)\n",
    "positions = run_mcmc(run_keys, n_draws, log_likelihood, (x1, y), initial_positions)\n",
    "positions.block_until_ready()\n",
    "\n",
    "trace3 = az.convert_to_inference_data({\"beta\": positions[:, 500:, :]})\n",
    "print(az.summary(trace3, var_names=[\"beta\"]))\n",
    "az.plot_trace(trace3, var_names=\"beta\", compact=False)\n",
    "az.plot_posterior(trace3, var_names=\"beta\", kind='kde')\n",
    "az.plot_forest(trace3, var_names=\"beta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Final remark\n",
    "\n",
    "In section 2.2 and 2.3, we have used a proposal kernel that has been tuned to the posterior density. Choosing the correct variance matrix is crucial... and difficult. In practice, we use an adaptive MCMC methods, that tunes the variance matrix along the iterations of the burn-in period. This is a bit too complex for this first year course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

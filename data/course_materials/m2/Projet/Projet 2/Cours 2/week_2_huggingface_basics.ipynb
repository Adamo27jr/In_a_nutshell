{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Hugging Face & Transformers: Using Pretrained Models\n",
    "\n",
    "This notebook walks through the essentials for **using Hugging Face**. We treat:\n",
    "- Selecting checkpoints and using pipelines\n",
    "- Manual tokenization and model forward passes\n",
    "- Generation parameters and devices\n",
    "- Batching with `datasets`\n",
    "- Caching, offline mode, and revisions\n",
    "- Optional: Hosted Inference API\n",
    "\n",
    "Use the opportunity to play and vary the different parameters of the model to get an idea on their influence on the outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb09f7d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install the core libraries (CPU by default). If you have a GPU, install the appropriate PyTorch build and optionally `bitsandbytes`.\n",
    "\n",
    "- Definition: An HF token is a personal key for accessing gated/private repos or hosted inference.\n",
    "- Why: Some models require accepting a license; hosted endpoints need to know who is calling.\n",
    "\n",
    "- Terminal: `pip install -U transformers datasets huggingface_hub accelerate safetensors`\n",
    "- GPU (optional): `pip install bitsandbytes` and a CUDA-enabled torch wheel.\n",
    "\n",
    "Authentication is only needed for gated/private repos or the hosted Inference API. You can either run `huggingface-cli login` or set `HF_TOKEN` in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4c69e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import accelerate\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96244122",
   "metadata": {},
   "source": [
    "## 1) Pipelines: Quick Inference\n",
    "- Definition: A pipeline bundles the right tokenizer, model, and postprocessing for a task.\n",
    "- Why: It reduces moving parts so you can confirm the model works before customizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e4fe40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998714923858643},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997186064720154}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment analysis (binary SST-2)\n",
    "sent_clf = pipeline(\n",
    "    'text-classification',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device_map='auto'\n",
    ")\n",
    "sent_clf(['I love data!', 'This is terrible...'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e84b814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9969370365142822,\n",
       "  'token': 3007,\n",
       "  'token_str': 'capital',\n",
       "  'sequence': 'paris is the capital of france.'},\n",
       " {'score': 0.0005914855282753706,\n",
       "  'token': 2540,\n",
       "  'token_str': 'heart',\n",
       "  'sequence': 'paris is the heart of france.'},\n",
       " {'score': 0.0004378748417366296,\n",
       "  'token': 2415,\n",
       "  'token_str': 'center',\n",
       "  'sequence': 'paris is the center of france.'},\n",
       " {'score': 0.0003378352848812938,\n",
       "  'token': 2803,\n",
       "  'token_str': 'centre',\n",
       "  'sequence': 'paris is the centre of france.'},\n",
       " {'score': 0.00026995810912922025,\n",
       "  'token': 2103,\n",
       "  'token_str': 'city',\n",
       "  'sequence': 'paris is the city of france.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill-mask\n",
    "mlm = pipeline('fill-mask', model='bert-base-uncased', device_map='auto')\n",
    "mlm('Paris is the [MASK] of France.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b204fdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Once upon a time, it was thought that even at the age of five, it would be hard to see a girl as attractive as you. But then, at thirteen, you were able to get to know her for'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text generation (small model for speed)\n",
    "gen = pipeline('text-generation', model='gpt2', device_map='auto')\n",
    "gen('Once upon a time', max_new_tokens=40, do_sample=True, temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a0903",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Use the version of gpt2 that was committed on Nov23, 20022, on Huggingface for the example above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eff14ba1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model gpt2 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1148, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: gpt2 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1148, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: gpt2 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16950/861653285.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Text generation avec une version spécifique de GPT-2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m gen = pipeline('text-generation', \n\u001b[0m\u001b[1;32m      3\u001b[0m                \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b5a36b5b5c5b5a5a5e5d5c5b5a59585756555453'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Remplacez par le hash réel du 23/11/2022\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                device_map='auto')\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         framework, model = infer_framework_load_model(\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0madapter_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0madapter_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mmodel_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_traceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"while loading with {class_name}, an error is thrown:\\n{trace}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    334\u001b[0m                 \u001b[0;34mf\"Could not load model {model} with any of the following classes: {class_tuple}. See the original errors:\\n\\n{error}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model gpt2 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1148, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: gpt2 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4900, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n  File \"/home/administrateur/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1148, in _get_resolved_checkpoint_files\n    raise OSError(\nOSError: gpt2 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n"
     ]
    }
   ],
   "source": [
    "# Text generation avec une version spécifique de GPT-2\n",
    "gen = pipeline('text-generation', \n",
    "               model='gpt2', \n",
    "               revision='b5a36b5b5c5b5a5a5e5d5c5b5a59585756555453',  # Remplacez par le hash réel du 23/11/2022\n",
    "               device_map='auto')\n",
    "gen('Once upon a time', max_new_tokens=40, do_sample=True, temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e401306e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Try `zero-shot-classification` with `facebook/bart-large-mnli`.\n",
    "- Try `summarization` with `facebook/bart-large-cnn` on a paragraph.\n",
    "- Try `feature-extraction` with `sentence-transformers/all-MiniLM-L6-v2` and compute cosine similarity between two sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487a191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573ce218a97c4867b6242d7dbcaecc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91a08da5f54448e8eaa20b6723f41be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-10-17T14:19:18.752655Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mReqwest(reqwest::Error { kind: Request, url: \"https://transfer.xethub.hf.co/xorbs/default/4266a5a099a58b225895b642647acff00ac0c3db4d2ea683f180a9736aeab27a?X-Xet-Signed-Range=bytes%3D29048743-31820655&X-Xet-Session-Id=01K7S8JKX6CVD0XK8HBQ9TA9WW&Expires=1760714310&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly90cmFuc2Zlci54ZXRodWIuaGYuY28veG9yYnMvZGVmYXVsdC80MjY2YTVhMDk5YTU4YjIyNTg5NWI2NDI2NDdhY2ZmMDBhYzBjM2RiNGQyZWE2ODNmMTgwYTk3MzZhZWFiMjdhP1gtWGV0LVNpZ25lZC1SYW5nZT1ieXRlcyUzRDI5MDQ4NzQzLTMxODIwNjU1JlgtWGV0LVNlc3Npb24tSWQ9MDFLN1M4SktYNkNWRDBYSzhIQlE5VEE5V1ciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjA3MTQzMTB9fX1dfQ__&Signature=Qo~DqQ0VrGhmbYXHg44La1SUiTD3vOUUzBbnz1Sn5Pi0~mtbFyaKbwgztHM8ZnddxLZ1Jtd1yoPgH7Gl7xWDKYyYI3xxU6ynDcbU1BjCF-qK8wXJ4i7UloJI4vrSnl8iuXuBj4SbYsiFUHQIdxlJeWeHJIKpSDawJfuKFA76S~aJrqhUZlbFczkW9qu-FhkldlZrSiMpUjpkwmTa13MJrPT5O6GKuW41e2Z5wSCMoGgNIKRkGCq4witFN14jNGyj16KTM1gnSxJLH3BuIbSsVWg~sqaXLonWFLNc9KFt3piKkeagZv0unDW-tD9uoBtmBDnlueGq3NDgXN~I4Fby3w__&Key-Pair-Id=K2L8F4GPSG1IFC\", source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(Io, Os { code: 110, kind: TimedOut, message: \"Connection timed out\" })) }). Retrying...\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:233\n",
      "\n",
      "  \u001b[2m2025-10-17T14:19:18.752795Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #0. Sleeping 1.293617004s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Zero-shot classification\n",
    "zero_shot = pipeline('zero-shot-classification', \n",
    "                    model='facebook/bart-large-mnli',\n",
    "                    device_map='auto')\n",
    "\n",
    "candidate_labels = ['technology', 'sports', 'politics', 'entertainment']\n",
    "text = \"The new smartphone has amazing features and incredible battery life.\"\n",
    "result = zero_shot(text, candidate_labels)\n",
    "print(\"Zero-shot classification:\")\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Predicted label: {result['labels'][0]} (confidence: {result['scores'][0]:.3f})\")\n",
    "print()\n",
    "\n",
    "# 2. Summarization\n",
    "summarizer = pipeline('summarization',\n",
    "                     model='facebook/bart-large-cnn',\n",
    "                     device_map='auto')\n",
    "\n",
    "article = '''\n",
    "Artificial intelligence is transforming many aspects of our daily lives. From virtual assistants \n",
    "like Siri and Alexa to recommendation systems on Netflix and Amazon, AI algorithms are becoming \n",
    "increasingly sophisticated. Machine learning models can now recognize images, translate languages, \n",
    "and even generate human-like text. However, these advancements also raise important ethical \n",
    "questions about privacy, bias, and job displacement that society must address.\n",
    "'''\n",
    "\n",
    "summary = summarizer(article, max_length=100, min_length=30, do_sample=False)\n",
    "print(\"Summarization:\")\n",
    "print(f\"Original length: {len(article)} characters\")\n",
    "print(f\"Summary: {summary[0]['summary_text']}\")\n",
    "print()\n",
    "\n",
    "# 3. Feature extraction and cosine similarity\n",
    "feature_extractor = pipeline('feature-extraction',\n",
    "                           model='sentence-transformers/all-MiniLM-L6-v2',\n",
    "                           device_map='auto')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "sentences = [\n",
    "    \"The weather is beautiful today\",\n",
    "    \"It's a lovely sunny day\",\n",
    "    \"I enjoy studying mathematics\"\n",
    "]\n",
    "\n",
    "# Extraire les features\n",
    "features = feature_extractor(sentences)\n",
    "\n",
    "# Calculer la similarité cosinus\n",
    "def get_sentence_embedding(features):\n",
    "    # Moyenner sur les tokens pour obtenir l'embedding de la phrase\n",
    "    return np.mean(features[0], axis=0)\n",
    "\n",
    "embeddings = [get_sentence_embedding(feat) for feat in features]\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "        print(f\"'{sentences[i][:20]}...' vs '{sentences[j][:20]}...': {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a849294",
   "metadata": {},
   "source": [
    "## 2) Manual Tokenization + Model Forward\n",
    "- Definition: A tokenizer maps text to token IDs and attention masks; a model head is a task-specific layer (e.g., classification).\n",
    "- Why: Manual control lets you batch, pad, and inspect outputs precisely for downstream evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc2be59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2140e-04, 9.9988e-01],\n",
       "        [9.9980e-01, 2.0337e-04]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "mdl = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "texts = [\n",
    "    'I absolutely loved this movie!',\n",
    "    'The plot was weak and boring.'\n",
    "]\n",
    "batch = tok(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    out = mdl(**batch)\n",
    "\n",
    "probs = out.logits.softmax(-1)\n",
    "probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd2e36",
   "metadata": {},
   "source": [
    "### Generation with `generate()`\n",
    "- Definition: Decoding chooses next tokens (sampling vs. beam search).\n",
    "- Why: Tuning decoding trades off creativity vs. determinism and repetition.\n",
    "Key parameters: `max_new_tokens`, `temperature`, `top_p`, `top_k`, `num_beams`, `repetition_penalty`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f943b080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In data science, transformers are the most common form of data manipulation, and have been used for centuries in many fields. They can be used to manipulate the data of many different types of applications.\n",
      "\n",
      "Transformers are a popular tool for data analysis, but they are not the only tool for data manipulation. In fact, the most common use of\n"
     ]
    }
   ],
   "source": [
    "lm_id = 'gpt2'  # small demo model\n",
    "tok_lm = AutoTokenizer.from_pretrained(lm_id)\n",
    "lm = AutoModelForCausalLM.from_pretrained(lm_id)\n",
    "inputs = tok_lm('In data science, transformers are', return_tensors='pt')\n",
    "out = lm.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "print(tok_lm.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc0d61",
   "metadata": {},
   "source": [
    "## 3) Devices and Memory\n",
    "- Definition: `device_map=\"auto\"` automatically places layers across CPU/GPU/MPS; `torch_dtype` sets numeric precision; quantization loads 8/4-bit weights.\n",
    "- Why: Fit models in memory and run them faster on your hardware.\n",
    "Use `device_map=\"auto\"` to place weights on available accelerators. Reduce memory via `torch_dtype=torch.float16` or 8/4-bit loading (requires `bitsandbytes`).\n",
    "\n",
    "### Experiment: benchmark dtype & device\n",
    "We compare runtime and memory across supported devices/dtypes. On CPU, float16 compute is usually not supported, so we skip it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "402ea341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device\tdtype\tparamMB\tsec(10 iters)\tpeakMB(cuda)\n",
      "cpu\tfloat32\t267.8\t1.157\tNone\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "texts = ['STUDYING DATA SCIENCE IS FUN!'] * 16\n",
    "\n",
    "devices = []\n",
    "if torch.cuda.is_available(): devices.append('cuda')\n",
    "if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():\n",
    "    devices.append('mps')\n",
    "devices.append('cpu')\n",
    "\n",
    "rows = []\n",
    "for device in devices:\n",
    "    for dtype in (torch.float32, torch.float16):\n",
    "        if device == 'cpu' and dtype is torch.float16:\n",
    "            continue\n",
    "        tok = AutoTokenizer.from_pretrained(name)\n",
    "        mdl = AutoModelForSequenceClassification.from_pretrained(name, dtype=dtype)\n",
    "        mdl.to(device)\n",
    "        batch = tok(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # Estimate parameter memory\n",
    "        param_mb = sum(p.numel() * p.element_size() for p in mdl.parameters()) / 1e6\n",
    "        # Optional: GPU peak memory\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        # Warmup + timed run\n",
    "        with torch.no_grad(): mdl(**batch)\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10): mdl(**batch)\n",
    "        dt = time.perf_counter() - t0\n",
    "        peak_mb = None\n",
    "        if device == 'cuda':\n",
    "            peak_mb = torch.cuda.max_memory_allocated() / 1e6\n",
    "        rows.append((device, str(dtype).replace('torch.', ''), round(param_mb,1), round(dt,3), None if peak_mb is None else round(peak_mb,1)))\n",
    "\n",
    "print('device\tdtype\tparamMB\tsec(10 iters)\tpeakMB(cuda)')\n",
    "for r in rows:\n",
    "    print('\t'.join(map(str, r)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69dfb5c",
   "metadata": {},
   "source": [
    "## 4) Understanding Batch Processing with Hugging Face Transformers\n",
    "The following code demonstrates a complete pipeline for processing and analyzing text data using Hugging Face's transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f8911d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14205beb7c8d4455940bea2503bb37b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08654aca995c473e87e36f78cf1a0345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecaa22457f1432091fe74398e822508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe5a4904bc84748960f6e42f833ee38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4e5bff6813490e9481157229738b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadef2bec0e142a2af3b08a513a49c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783fa64dbefc4d94b08a2a11543c5160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9995173024c841dfbff6a12c5a8619ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08fc3a7f7754666ab5e3dcc60020cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       "  \"Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\",\n",
       "  \"its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\",\n",
       "  \"STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *\",\n",
       "  \"First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\"],\n",
       " 'label': [0, 0, 0, 0, 0],\n",
       " 'input_ids': [[101,\n",
       "   1045,\n",
       "   2293,\n",
       "   16596,\n",
       "   1011,\n",
       "   10882,\n",
       "   1998,\n",
       "   2572,\n",
       "   5627,\n",
       "   2000,\n",
       "   2404,\n",
       "   2039,\n",
       "   2007,\n",
       "   1037,\n",
       "   2843,\n",
       "   1012,\n",
       "   16596,\n",
       "   1011,\n",
       "   10882,\n",
       "   5691,\n",
       "   1013,\n",
       "   2694,\n",
       "   2024,\n",
       "   2788,\n",
       "   2104,\n",
       "   11263,\n",
       "   25848,\n",
       "   1010,\n",
       "   2104,\n",
       "   1011,\n",
       "   12315,\n",
       "   1998,\n",
       "   28947,\n",
       "   1012,\n",
       "   1045,\n",
       "   2699,\n",
       "   2000,\n",
       "   2066,\n",
       "   2023,\n",
       "   1010,\n",
       "   1045,\n",
       "   2428,\n",
       "   2106,\n",
       "   1010,\n",
       "   2021,\n",
       "   2009,\n",
       "   2003,\n",
       "   2000,\n",
       "   2204,\n",
       "   2694,\n",
       "   16596,\n",
       "   1011,\n",
       "   10882,\n",
       "   2004,\n",
       "   17690,\n",
       "   1019,\n",
       "   2003,\n",
       "   2000,\n",
       "   2732,\n",
       "   10313,\n",
       "   1006,\n",
       "   1996,\n",
       "   2434,\n",
       "   1007,\n",
       "   1012,\n",
       "   10021,\n",
       "   4013,\n",
       "   3367,\n",
       "   20086,\n",
       "   2015,\n",
       "   1010,\n",
       "   10036,\n",
       "   19747,\n",
       "   4520,\n",
       "   1010,\n",
       "   25931,\n",
       "   3064,\n",
       "   22580,\n",
       "   1010,\n",
       "   1039,\n",
       "   2290,\n",
       "   2008,\n",
       "   2987,\n",
       "   1005,\n",
       "   1056,\n",
       "   2674,\n",
       "   1996,\n",
       "   4281,\n",
       "   1010,\n",
       "   1998,\n",
       "   16267,\n",
       "   2028,\n",
       "   1011,\n",
       "   8789,\n",
       "   3494,\n",
       "   3685,\n",
       "   2022,\n",
       "   9462,\n",
       "   2007,\n",
       "   1037,\n",
       "   1005,\n",
       "   16596,\n",
       "   1011,\n",
       "   10882,\n",
       "   1005,\n",
       "   4292,\n",
       "   1012,\n",
       "   1006,\n",
       "   1045,\n",
       "   1005,\n",
       "   1049,\n",
       "   2469,\n",
       "   2045,\n",
       "   2024,\n",
       "   2216,\n",
       "   1997,\n",
       "   2017,\n",
       "   2041,\n",
       "   2045,\n",
       "   2040,\n",
       "   2228,\n",
       "   17690,\n",
       "   1019,\n",
       "   2003,\n",
       "   2204,\n",
       "   16596,\n",
       "   1011,\n",
       "   10882,\n",
       "   2694,\n",
       "   1012,\n",
       "   2009,\n",
       "   1005,\n",
       "   1055,\n",
       "   2025,\n",
       "   1012,\n",
       "   2009,\n",
       "   1005,\n",
       "   1055,\n",
       "   18856,\n",
       "   17322,\n",
       "   2094,\n",
       "   1998,\n",
       "   4895,\n",
       "   7076,\n",
       "   8197,\n",
       "   4892,\n",
       "   1012,\n",
       "   1007,\n",
       "   2096,\n",
       "   2149,\n",
       "   7193,\n",
       "   2453,\n",
       "   2066,\n",
       "   7603,\n",
       "   1998,\n",
       "   2839,\n",
       "   2458,\n",
       "   1010,\n",
       "   16596,\n",
       "   1011,\n",
       "   10882,\n",
       "   2003,\n",
       "   1037,\n",
       "   6907,\n",
       "   2008,\n",
       "   2515,\n",
       "   2025,\n",
       "   2202,\n",
       "   2993,\n",
       "   5667,\n",
       "   1006,\n",
       "   12935,\n",
       "   1012,\n",
       "   2732,\n",
       "   10313,\n",
       "   1007,\n",
       "   1012,\n",
       "   2009,\n",
       "   2089,\n",
       "   7438,\n",
       "   2590,\n",
       "   3314,\n",
       "   1010,\n",
       "   2664,\n",
       "   2025,\n",
       "   2004,\n",
       "   1037,\n",
       "   3809,\n",
       "   4695,\n",
       "   1012,\n",
       "   2009,\n",
       "   1005,\n",
       "   1055,\n",
       "   2428,\n",
       "   3697,\n",
       "   2000,\n",
       "   2729,\n",
       "   2055,\n",
       "   1996,\n",
       "   3494,\n",
       "   2182,\n",
       "   2004,\n",
       "   2027,\n",
       "   2024,\n",
       "   2025,\n",
       "   3432,\n",
       "   13219,\n",
       "   1010,\n",
       "   2074,\n",
       "   4394,\n",
       "   1037,\n",
       "   12125,\n",
       "   1997,\n",
       "   2166,\n",
       "   1012,\n",
       "   2037,\n",
       "   4506,\n",
       "   1998,\n",
       "   9597,\n",
       "   2024,\n",
       "   4799,\n",
       "   1998,\n",
       "   21425,\n",
       "   1010,\n",
       "   2411,\n",
       "   9145,\n",
       "   2000,\n",
       "   3422,\n",
       "   1012,\n",
       "   1996,\n",
       "   11153,\n",
       "   1997,\n",
       "   3011,\n",
       "   2113,\n",
       "   2009,\n",
       "   1005,\n",
       "   1055,\n",
       "   29132,\n",
       "   2004,\n",
       "   2027,\n",
       "   2031,\n",
       "   2000,\n",
       "   2467,\n",
       "   2360,\n",
       "   1000,\n",
       "   4962,\n",
       "   8473,\n",
       "   4181,\n",
       "   9766,\n",
       "   1005,\n",
       "   1055,\n",
       "   3011,\n",
       "   1012,\n",
       "   1012,\n",
       "   1012,\n",
       "   1000,\n",
       "   4728,\n",
       "   2111,\n",
       "   2052,\n",
       "   2025,\n",
       "   3613,\n",
       "   3666,\n",
       "   1012,\n",
       "   8473,\n",
       "   4181,\n",
       "   9766,\n",
       "   1005,\n",
       "   1055,\n",
       "   11289,\n",
       "   2442,\n",
       "   2022,\n",
       "   3810,\n",
       "   1999,\n",
       "   2037,\n",
       "   8753,\n",
       "   2004,\n",
       "   2023,\n",
       "   10634,\n",
       "   1010,\n",
       "   10036,\n",
       "   1010,\n",
       "   9996,\n",
       "   5493,\n",
       "   1006,\n",
       "   3666,\n",
       "   2009,\n",
       "   2302,\n",
       "   4748,\n",
       "   16874,\n",
       "   7807,\n",
       "   2428,\n",
       "   7545,\n",
       "   2023,\n",
       "   2188,\n",
       "   1007,\n",
       "   19817,\n",
       "   6784,\n",
       "   4726,\n",
       "   19817,\n",
       "   19736,\n",
       "   3372,\n",
       "   1997,\n",
       "   1037,\n",
       "   2265,\n",
       "   13891,\n",
       "   2015,\n",
       "   2046,\n",
       "   2686,\n",
       "   1012,\n",
       "   27594,\n",
       "   2121,\n",
       "   1012,\n",
       "   2061,\n",
       "   1010,\n",
       "   3102,\n",
       "   2125,\n",
       "   1037,\n",
       "   2364,\n",
       "   2839,\n",
       "   1012,\n",
       "   1998,\n",
       "   2059,\n",
       "   3288,\n",
       "   2032,\n",
       "   2067,\n",
       "   2004,\n",
       "   2178,\n",
       "   3364,\n",
       "   1012,\n",
       "   15333,\n",
       "   4402,\n",
       "   2480,\n",
       "   999,\n",
       "   5759,\n",
       "   2035,\n",
       "   2058,\n",
       "   2153,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   4276,\n",
       "   1996,\n",
       "   4024,\n",
       "   3643,\n",
       "   1997,\n",
       "   1037,\n",
       "   12635,\n",
       "   1010,\n",
       "   2926,\n",
       "   2065,\n",
       "   2017,\n",
       "   2066,\n",
       "   2895,\n",
       "   5691,\n",
       "   1012,\n",
       "   2023,\n",
       "   2028,\n",
       "   2838,\n",
       "   1996,\n",
       "   5156,\n",
       "   2482,\n",
       "   29515,\n",
       "   1010,\n",
       "   9590,\n",
       "   2007,\n",
       "   1996,\n",
       "   2307,\n",
       "   3158,\n",
       "   5477,\n",
       "   4168,\n",
       "   5926,\n",
       "   2806,\n",
       "   1010,\n",
       "   5008,\n",
       "   7465,\n",
       "   2007,\n",
       "   1996,\n",
       "   2871,\n",
       "   5806,\n",
       "   7170,\n",
       "   13305,\n",
       "   1010,\n",
       "   1998,\n",
       "   2130,\n",
       "   9452,\n",
       "   2806,\n",
       "   9767,\n",
       "   1012,\n",
       "   2035,\n",
       "   1997,\n",
       "   2023,\n",
       "   2003,\n",
       "   14036,\n",
       "   1998,\n",
       "   17824,\n",
       "   2135,\n",
       "   8971,\n",
       "   2021,\n",
       "   2045,\n",
       "   2003,\n",
       "   2498,\n",
       "   2008,\n",
       "   2428,\n",
       "   13783,\n",
       "   2017,\n",
       "   2185,\n",
       "   2065,\n",
       "   2017,\n",
       "   1005,\n",
       "   2310,\n",
       "   2464,\n",
       "   2115,\n",
       "   3745,\n",
       "   2077,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1996,\n",
       "   5436,\n",
       "   2003,\n",
       "   2081,\n",
       "   5875,\n",
       "   2011,\n",
       "   1996,\n",
       "   10502,\n",
       "   1997,\n",
       "   1037,\n",
       "   10442,\n",
       "   1010,\n",
       "   2029,\n",
       "   2003,\n",
       "   12266,\n",
       "   2021,\n",
       "   6684,\n",
       "   13769,\n",
       "   1012,\n",
       "   2116,\n",
       "   1997,\n",
       "   1996,\n",
       "   3494,\n",
       "   2024,\n",
       "   4600,\n",
       "   12991,\n",
       "   13874,\n",
       "   2094,\n",
       "   1011,\n",
       "   1011,\n",
       "   1996,\n",
       "   4854,\n",
       "   8244,\n",
       "   1010,\n",
       "   1996,\n",
       "   10215,\n",
       "   6206,\n",
       "   12114,\n",
       "   1010,\n",
       "   1996,\n",
       "   15274,\n",
       "   10558,\n",
       "   1010,\n",
       "   1996,\n",
       "   24436,\n",
       "   7349,\n",
       "   2015,\n",
       "   1010,\n",
       "   1996,\n",
       "   7743,\n",
       "   2100,\n",
       "   7823,\n",
       "   3203,\n",
       "   2276,\n",
       "   2132,\n",
       "   1010,\n",
       "   1996,\n",
       "   15274,\n",
       "   3761,\n",
       "   1010,\n",
       "   1996,\n",
       "   6638,\n",
       "   2976,\n",
       "   2063,\n",
       "   2040,\n",
       "   3504,\n",
       "   2066,\n",
       "   2002,\n",
       "   2001,\n",
       "   2828,\n",
       "   10526,\n",
       "   2004,\n",
       "   1996,\n",
       "   4916,\n",
       "   1999,\n",
       "   1037,\n",
       "   5365,\n",
       "   3185,\n",
       "   2013,\n",
       "   1996,\n",
       "   7675,\n",
       "   1012,\n",
       "   2035,\n",
       "   3413,\n",
       "   8231,\n",
       "   6051,\n",
       "   2021,\n",
       "   2153,\n",
       "   2498,\n",
       "   2569,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1045,\n",
       "   2245,\n",
       "   1996,\n",
       "   2364,\n",
       "   16219,\n",
       "   2020,\n",
       "   3492,\n",
       "   2092,\n",
       "   2589,\n",
       "   1998,\n",
       "   7199,\n",
       "   2092,\n",
       "   6051,\n",
       "   1012,\n",
       "   2011,\n",
       "   1996,\n",
       "   2203,\n",
       "   1997,\n",
       "   1996,\n",
       "   3185,\n",
       "   2017,\n",
       "   5121,\n",
       "   2354,\n",
       "   2040,\n",
       "   1996,\n",
       "   2204,\n",
       "   4364,\n",
       "   2020,\n",
       "   1998,\n",
       "   4694,\n",
       "   1005,\n",
       "   1056,\n",
       "   1012,\n",
       "   2045,\n",
       "   2001,\n",
       "   2019,\n",
       "   6832,\n",
       "   6336,\n",
       "   2004,\n",
       "   1996,\n",
       "   2428,\n",
       "   2919,\n",
       "   3924,\n",
       "   2288,\n",
       "   2037,\n",
       "   2074,\n",
       "   28858,\n",
       "   1012,\n",
       "   2200,\n",
       "   21934,\n",
       "   24759,\n",
       "   6553,\n",
       "   1010,\n",
       "   2021,\n",
       "   2059,\n",
       "   2017,\n",
       "   4694,\n",
       "   1005,\n",
       "   1056,\n",
       "   8074,\n",
       "   8429,\n",
       "   1010,\n",
       "   2157,\n",
       "   1029,\n",
       "   1996,\n",
       "   2069,\n",
       "   2518,\n",
       "   1045,\n",
       "   2179,\n",
       "   2428,\n",
       "   15703,\n",
       "   2001,\n",
       "   1996,\n",
       "   5377,\n",
       "   7659,\n",
       "   2000,\n",
       "   1058,\n",
       "   5104,\n",
       "   2684,\n",
       "   2076,\n",
       "   1996,\n",
       "   2197,\n",
       "   2954,\n",
       "   3496,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2025,\n",
       "   2919,\n",
       "   1012,\n",
       "   2025,\n",
       "   2204,\n",
       "   1012,\n",
       "   3413,\n",
       "   3085,\n",
       "   1018,\n",
       "   1012,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   2049,\n",
       "   1037,\n",
       "   6135,\n",
       "   2779,\n",
       "   2143,\n",
       "   2007,\n",
       "   1037,\n",
       "   2261,\n",
       "   4100,\n",
       "   1011,\n",
       "   10303,\n",
       "   2895,\n",
       "   10071,\n",
       "   2008,\n",
       "   2191,\n",
       "   1996,\n",
       "   5436,\n",
       "   4025,\n",
       "   1037,\n",
       "   2210,\n",
       "   2488,\n",
       "   1998,\n",
       "   10825,\n",
       "   1996,\n",
       "   13972,\n",
       "   1997,\n",
       "   1996,\n",
       "   4438,\n",
       "   3158,\n",
       "   5477,\n",
       "   3152,\n",
       "   1012,\n",
       "   3033,\n",
       "   1997,\n",
       "   1996,\n",
       "   5436,\n",
       "   2123,\n",
       "   1005,\n",
       "   1056,\n",
       "   2191,\n",
       "   3168,\n",
       "   1998,\n",
       "   4025,\n",
       "   2000,\n",
       "   2022,\n",
       "   2794,\n",
       "   1999,\n",
       "   2000,\n",
       "   2224,\n",
       "   2039,\n",
       "   2051,\n",
       "   1012,\n",
       "   1996,\n",
       "   2203,\n",
       "   5436,\n",
       "   2003,\n",
       "   2008,\n",
       "   1997,\n",
       "   1037,\n",
       "   2200,\n",
       "   3937,\n",
       "   2828,\n",
       "   2008,\n",
       "   2987,\n",
       "   1005,\n",
       "   1056,\n",
       "   2681,\n",
       "   1996,\n",
       "   13972,\n",
       "   16986,\n",
       "   1998,\n",
       "   2151,\n",
       "   21438,\n",
       "   2024,\n",
       "   5793,\n",
       "   2013,\n",
       "   1996,\n",
       "   2927,\n",
       "   1012,\n",
       "   1996,\n",
       "   2203,\n",
       "   3496,\n",
       "   2007,\n",
       "   1996,\n",
       "   13109,\n",
       "   19895,\n",
       "   10457,\n",
       "   2123,\n",
       "   1005,\n",
       "   1056,\n",
       "   2191,\n",
       "   3168,\n",
       "   2004,\n",
       "   2027,\n",
       "   2024,\n",
       "   2794,\n",
       "   1999,\n",
       "   1998,\n",
       "   4025,\n",
       "   2000,\n",
       "   2031,\n",
       "   2210,\n",
       "   21923,\n",
       "   2000,\n",
       "   1996,\n",
       "   2381,\n",
       "   1997,\n",
       "   3158,\n",
       "   5477,\n",
       "   1005,\n",
       "   1055,\n",
       "   2839,\n",
       "   1012,\n",
       "   2025,\n",
       "   2428,\n",
       "   4276,\n",
       "   3666,\n",
       "   2153,\n",
       "   1010,\n",
       "   2978,\n",
       "   9364,\n",
       "   1999,\n",
       "   1996,\n",
       "   2203,\n",
       "   2537,\n",
       "   1010,\n",
       "   2130,\n",
       "   2295,\n",
       "   2009,\n",
       "   2003,\n",
       "   6835,\n",
       "   2009,\n",
       "   2001,\n",
       "   2915,\n",
       "   2006,\n",
       "   1037,\n",
       "   2659,\n",
       "   5166,\n",
       "   3056,\n",
       "   7171,\n",
       "   1998,\n",
       "   5433,\n",
       "   1999,\n",
       "   1996,\n",
       "   2143,\n",
       "   2024,\n",
       "   1997,\n",
       "   3532,\n",
       "   2856,\n",
       "   3737,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   2732,\n",
       "   5790,\n",
       "   1024,\n",
       "   1008,\n",
       "   1008,\n",
       "   1008,\n",
       "   1008,\n",
       "   1008,\n",
       "   5095,\n",
       "   2305,\n",
       "   1008,\n",
       "   1008,\n",
       "   1008,\n",
       "   1008,\n",
       "   5958,\n",
       "   2305,\n",
       "   1008,\n",
       "   1008,\n",
       "   1008,\n",
       "   5958,\n",
       "   2851,\n",
       "   1008,\n",
       "   1008,\n",
       "   4465,\n",
       "   2305,\n",
       "   1008,\n",
       "   6928,\n",
       "   2851,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2280,\n",
       "   2047,\n",
       "   5979,\n",
       "   18268,\n",
       "   8872,\n",
       "   2990,\n",
       "   6487,\n",
       "   5178,\n",
       "   13754,\n",
       "   1006,\n",
       "   3744,\n",
       "   8149,\n",
       "   3158,\n",
       "   5477,\n",
       "   4168,\n",
       "   1007,\n",
       "   2003,\n",
       "   2128,\n",
       "   1011,\n",
       "   4137,\n",
       "   2000,\n",
       "   8912,\n",
       "   1010,\n",
       "   1037,\n",
       "   2235,\n",
       "   2021,\n",
       "   6355,\n",
       "   2237,\n",
       "   1999,\n",
       "   3290,\n",
       "   2000,\n",
       "   2393,\n",
       "   1996,\n",
       "   2610,\n",
       "   2045,\n",
       "   2007,\n",
       "   2037,\n",
       "   4073,\n",
       "   2000,\n",
       "   2644,\n",
       "   1037,\n",
       "   2350,\n",
       "   19690,\n",
       "   19535,\n",
       "   3169,\n",
       "   2046,\n",
       "   2037,\n",
       "   2237,\n",
       "   1012,\n",
       "   1996,\n",
       "   12731,\n",
       "   14277,\n",
       "   14778,\n",
       "   2015,\n",
       "   2735,\n",
       "   2041,\n",
       "   2000,\n",
       "   2022,\n",
       "   4654,\n",
       "   1011,\n",
       "   2510,\n",
       "   1010,\n",
       "   2599,\n",
       "   2011,\n",
       "   2280,\n",
       "   3474,\n",
       "   6425,\n",
       "   11527,\n",
       "   2015,\n",
       "   1006,\n",
       "   4459,\n",
       "   2935,\n",
       "   1010,\n",
       "   4728,\n",
       "   2124,\n",
       "   2004,\n",
       "   18626,\n",
       "   2013,\n",
       "   2264,\n",
       "   2203,\n",
       "   2545,\n",
       "   1007,\n",
       "   2040,\n",
       "   2003,\n",
       "   2478,\n",
       "   1037,\n",
       "   2569,\n",
       "   4118,\n",
       "   2002,\n",
       "   4342,\n",
       "   1999,\n",
       "   7041,\n",
       "   2000,\n",
       "   2954,\n",
       "   2125,\n",
       "   2010,\n",
       "   7892,\n",
       "   1012,\n",
       "   2021,\n",
       "   2990,\n",
       "   2038,\n",
       "   1037,\n",
       "   2062,\n",
       "   3167,\n",
       "   3114,\n",
       "   2005,\n",
       "   2635,\n",
       "   2032,\n",
       "   2091,\n",
       "   1010,\n",
       "   2008,\n",
       "   9891,\n",
       "   1996,\n",
       "   2048,\n",
       "   2273,\n",
       "   2046,\n",
       "   2019,\n",
       "   11355,\n",
       "   2345,\n",
       "   24419,\n",
       "   2073,\n",
       "   2069,\n",
       "   2028,\n",
       "   2097,\n",
       "   3328,\n",
       "   2185,\n",
       "   4142,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2044,\n",
       "   2127,\n",
       "   2331,\n",
       "   1010,\n",
       "   3158,\n",
       "   5477,\n",
       "   4168,\n",
       "   2596,\n",
       "   2000,\n",
       "   2022,\n",
       "   2006,\n",
       "   1037,\n",
       "   2152,\n",
       "   1010,\n",
       "   4760,\n",
       "   2002,\n",
       "   2071,\n",
       "   2191,\n",
       "   1996,\n",
       "   2190,\n",
       "   3442,\n",
       "   2000,\n",
       "   2678,\n",
       "   3152,\n",
       "   1999,\n",
       "   1996,\n",
       "   2895,\n",
       "   3006,\n",
       "   1012,\n",
       "   2096,\n",
       "   2008,\n",
       "   2001,\n",
       "   1037,\n",
       "   2521,\n",
       "   2062,\n",
       "   3689,\n",
       "   8048,\n",
       "   2143,\n",
       "   1010,\n",
       "   2007,\n",
       "   1996,\n",
       "   11133,\n",
       "   2002,\n",
       "   2038,\n",
       "   2513,\n",
       "   2000,\n",
       "   1996,\n",
       "   2152,\n",
       "   1011,\n",
       "   10209,\n",
       "   1010,\n",
       "   2053,\n",
       "   4167,\n",
       "   2121,\n",
       "   2895,\n",
       "   2008,\n",
       "   2034,\n",
       "   2081,\n",
       "   2032,\n",
       "   3297,\n",
       "   1998,\n",
       "   2038,\n",
       "   13718,\n",
       "   2550,\n",
       "   2010,\n",
       "   5409,\n",
       "   2143,\n",
       "   2144,\n",
       "   4315,\n",
       "   17440,\n",
       "   1012,\n",
       "   2009,\n",
       "   1005,\n",
       "   1055,\n",
       "   7880,\n",
       "   2379,\n",
       "   2004,\n",
       "   2919,\n",
       "   2004,\n",
       "   2008,\n",
       "   2143,\n",
       "   1010,\n",
       "   2021,\n",
       "   2054,\n",
       "   1045,\n",
       "   2056,\n",
       "   2145,\n",
       "   4832,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1037,\n",
       "   10634,\n",
       "   1010,\n",
       "   21425,\n",
       "   2143,\n",
       "   1010,\n",
       "   2007,\n",
       "   2200,\n",
       "   2210,\n",
       "   1999,\n",
       "   1996,\n",
       "   2126,\n",
       "   1997,\n",
       "   2151,\n",
       "   10990,\n",
       "   2895,\n",
       "   1012,\n",
       "   2054,\n",
       "   2210,\n",
       "   2045,\n",
       "   2003,\n",
       "   3701,\n",
       "   3774,\n",
       "   1997,\n",
       "   2070,\n",
       "   14401,\n",
       "   2954,\n",
       "   5019,\n",
       "   1010,\n",
       "   2667,\n",
       "   2000,\n",
       "   2298,\n",
       "   4658,\n",
       "   1998,\n",
       "   9874,\n",
       "   2100,\n",
       "   2007,\n",
       "   2070,\n",
       "   10036,\n",
       "   22889,\n",
       "   2080,\n",
       "   1011,\n",
       "   9587,\n",
       "   1013,\n",
       "   16887,\n",
       "   2039,\n",
       "   3896,\n",
       "   2794,\n",
       "   2000,\n",
       "   2068,\n",
       "   2008,\n",
       "   13718,\n",
       "   2612,\n",
       "   2191,\n",
       "   2068,\n",
       "   2298,\n",
       "   2062,\n",
       "   7143,\n",
       "   1012,\n",
       "   2108,\n",
       "   1037,\n",
       "   4916,\n",
       "   2275,\n",
       "   2143,\n",
       "   1010,\n",
       "   2472,\n",
       "   7527,\n",
       "   13109,\n",
       "   5686,\n",
       "   26730,\n",
       "   2038,\n",
       "   2699,\n",
       "   2000,\n",
       "   2507,\n",
       "   1996,\n",
       "   2143,\n",
       "   1037,\n",
       "   2728,\n",
       "   9172,\n",
       "   1013,\n",
       "   4078,\n",
       "   4842,\n",
       "   9365,\n",
       "   4066,\n",
       "   1997,\n",
       "   2514,\n",
       "   1010,\n",
       "   2021,\n",
       "   2023,\n",
       "   2069,\n",
       "   9909,\n",
       "   2000,\n",
       "   1996,\n",
       "   15561,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1058,\n",
       "   2094,\n",
       "   3957,\n",
       "   1037,\n",
       "   3391,\n",
       "   4895,\n",
       "   7076,\n",
       "   21649,\n",
       "   2836,\n",
       "   1998,\n",
       "   2445,\n",
       "   2002,\n",
       "   1005,\n",
       "   1055,\n",
       "   2196,\n",
       "   2042,\n",
       "   1037,\n",
       "   2728,\n",
       "   2139,\n",
       "   9152,\n",
       "   3217,\n",
       "   4066,\n",
       "   1997,\n",
       "   3364,\n",
       "   1010,\n",
       "   2008,\n",
       "   2064,\n",
       "   1005,\n",
       "   1056,\n",
       "   2022,\n",
       "   2204,\n",
       "   1012,\n",
       "   2004,\n",
       "   1996,\n",
       "   12700,\n",
       "   1010,\n",
       "   2935,\n",
       "   5807,\n",
       "   1005,\n",
       "   1056,\n",
       "   5987,\n",
       "   2000,\n",
       "   2681,\n",
       "   1996,\n",
       "   10506,\n",
       "   2497,\n",
       "   15933,\n",
       "   2574,\n",
       "   1012,\n",
       "   2002,\n",
       "   4152,\n",
       "   2210,\n",
       "   7982,\n",
       "   2012,\n",
       "   1996,\n",
       "   2927,\n",
       "   2004,\n",
       "   2002,\n",
       "   11785,\n",
       "   2000,\n",
       "   20327,\n",
       "   2019,\n",
       "   2137,\n",
       "   9669,\n",
       "   2021,\n",
       "   4152,\n",
       "   29239,\n",
       "   2488,\n",
       "   2875,\n",
       "   1996,\n",
       "   2203,\n",
       "   1012,\n",
       "   2035,\n",
       "   1996,\n",
       "   4637,\n",
       "   3459,\n",
       "   2024,\n",
       "   8053,\n",
       "   20857,\n",
       "   1010,\n",
       "   1998,\n",
       "   2079,\n",
       "   2498,\n",
       "   2000,\n",
       "   5333,\n",
       "   1996,\n",
       "   3152,\n",
       "   8633,\n",
       "   2012,\n",
       "   2035,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2023,\n",
       "   2003,\n",
       "   2028,\n",
       "   11133,\n",
       "   2008,\n",
       "   1005,\n",
       "   1055,\n",
       "   15926,\n",
       "   2098,\n",
       "   2157,\n",
       "   2013,\n",
       "   1996,\n",
       "   19311,\n",
       "   1012,\n",
       "   1008,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [101,\n",
       "   2034,\n",
       "   2125,\n",
       "   2292,\n",
       "   2033,\n",
       "   2360,\n",
       "   1010,\n",
       "   2065,\n",
       "   2017,\n",
       "   4033,\n",
       "   1005,\n",
       "   1056,\n",
       "   5632,\n",
       "   1037,\n",
       "   3158,\n",
       "   5477,\n",
       "   4168,\n",
       "   3185,\n",
       "   2144,\n",
       "   2668,\n",
       "   20205,\n",
       "   1010,\n",
       "   2017,\n",
       "   2763,\n",
       "   2097,\n",
       "   2025,\n",
       "   2066,\n",
       "   2023,\n",
       "   3185,\n",
       "   1012,\n",
       "   2087,\n",
       "   1997,\n",
       "   2122,\n",
       "   5691,\n",
       "   2089,\n",
       "   2025,\n",
       "   2031,\n",
       "   1996,\n",
       "   2190,\n",
       "   14811,\n",
       "   2030,\n",
       "   2190,\n",
       "   5889,\n",
       "   2021,\n",
       "   1045,\n",
       "   5959,\n",
       "   2122,\n",
       "   7957,\n",
       "   1997,\n",
       "   5691,\n",
       "   2005,\n",
       "   2054,\n",
       "   2027,\n",
       "   2024,\n",
       "   1012,\n",
       "   2023,\n",
       "   3185,\n",
       "   2003,\n",
       "   2172,\n",
       "   2488,\n",
       "   2084,\n",
       "   2151,\n",
       "   1997,\n",
       "   1996,\n",
       "   5691,\n",
       "   1996,\n",
       "   2060,\n",
       "   2895,\n",
       "   4364,\n",
       "   1006,\n",
       "   16562,\n",
       "   2140,\n",
       "   1998,\n",
       "   2079,\n",
       "   14277,\n",
       "   2232,\n",
       "   1007,\n",
       "   2031,\n",
       "   2245,\n",
       "   2055,\n",
       "   5128,\n",
       "   2041,\n",
       "   1996,\n",
       "   2627,\n",
       "   2261,\n",
       "   2086,\n",
       "   1012,\n",
       "   3158,\n",
       "   5477,\n",
       "   4168,\n",
       "   2003,\n",
       "   2204,\n",
       "   1999,\n",
       "   1996,\n",
       "   3185,\n",
       "   1010,\n",
       "   1996,\n",
       "   3185,\n",
       "   2003,\n",
       "   2069,\n",
       "   4276,\n",
       "   3666,\n",
       "   2000,\n",
       "   3158,\n",
       "   5477,\n",
       "   4168,\n",
       "   4599,\n",
       "   1012,\n",
       "   2009,\n",
       "   2003,\n",
       "   2025,\n",
       "   2004,\n",
       "   2204,\n",
       "   2004,\n",
       "   5256,\n",
       "   1997,\n",
       "   2331,\n",
       "   1006,\n",
       "   2029,\n",
       "   1045,\n",
       "   3811,\n",
       "   16755,\n",
       "   2000,\n",
       "   3087,\n",
       "   1997,\n",
       "   7777,\n",
       "   3158,\n",
       "   5477,\n",
       "   4168,\n",
       "   1007,\n",
       "   2030,\n",
       "   1999,\n",
       "   3109,\n",
       "   2021,\n",
       "   1010,\n",
       "   1999,\n",
       "   2026,\n",
       "   5448,\n",
       "   2009,\n",
       "   1005,\n",
       "   1055,\n",
       "   4276,\n",
       "   3666,\n",
       "   1012,\n",
       "   2009,\n",
       "   2038,\n",
       "   1996,\n",
       "   2168,\n",
       "   2828,\n",
       "   1997,\n",
       "   2514,\n",
       "   2000,\n",
       "   2009,\n",
       "   2004,\n",
       "   7880,\n",
       "   2000,\n",
       "   2448,\n",
       "   1012,\n",
       "   2204,\n",
       "   4569,\n",
       "   4933,\n",
       "   999,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'pred': [0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a dataset \n",
    "ds = load_dataset('imdb', split='test[:2%]')\n",
    "# Tokenizer and model for sequence classification\n",
    "tok_cls = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "mdl_cls = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', device_map='auto')\n",
    "\n",
    "# Preprocessing function to tokenize text\n",
    "def preprocess(batch):\n",
    "    return tok_cls(batch['text'], truncation=True, padding=True)\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "ds_tok = ds.map(preprocess, batched=True)\n",
    "\n",
    "# Prediction function to run the model and get predictions\n",
    "def predict(batch):\n",
    "    # Extract relevant keys and convert to tensors\n",
    "    keep = {k: batch[k] for k in ['input_ids', 'attention_mask'] if k in batch}\n",
    "    # Move tensors to the same device as the model\n",
    "    tens = {k: torch.tensor(v).to(mdl_cls.device) for k, v in keep.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = mdl_cls(**tens)\n",
    "        # Move predictions back to CPU for dataset storage\n",
    "        predictions = outputs.logits.argmax(-1).cpu().tolist()\n",
    "    \n",
    "    return {'pred': predictions}\n",
    "\n",
    "# Run predictions with smaller batch size for stability\n",
    "preds = ds_tok.map(predict, batched=True, batch_size=8)\n",
    "preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7a67c",
   "metadata": {},
   "source": [
    "## 5) Revisions, Caching, and Offline\n",
    "- Definition: A checkpoint is a released model; a revision is an exact commit/tag.\n",
    "- Why: Pinning revisions and controlling caches ensures reproducibility on different machines.\n",
    "- Pin exact versions with `revision=` when calling `from_pretrained`.\n",
    "- Set cache directories with `HF_HOME` or `TRANSFORMERS_CACHE`.\n",
    "- Force offline mode with `HF_HUB_OFFLINE=1` (uses only local cache).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01c1f243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME = None\n",
      "TRANSFORMERS_CACHE = None\n"
     ]
    }
   ],
   "source": [
    "# Example: pinning a revision (replace with a real commit SHA/tag for production)\n",
    "tok_pinned = AutoTokenizer.from_pretrained('bert-base-uncased', revision='main')\n",
    "mdl_pinned = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', revision='main')\n",
    "\n",
    "# Where is the cache?\n",
    "print('HF_HOME =', os.getenv('HF_HOME'))\n",
    "print('TRANSFORMERS_CACHE =', os.getenv('TRANSFORMERS_CACHE'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eae6b11",
   "metadata": {},
   "source": [
    "## 6) Optional: Hosted Inference API\n",
    "- Definition: The Inference API is a managed endpoint for common tasks.\n",
    "- Why: Zero setup for quick demos or when you lack local GPU resources.\n",
    "Run inference on HF-hosted models (requires token; rate limits apply).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da7cdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN not set; skipping hosted inference.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from huggingface_hub import InferenceClient\n",
    "    if HF_TOKEN:\n",
    "        client = InferenceClient(model='facebook/bart-large-cnn', token=HF_TOKEN)\n",
    "        s = client.summarization('''Large Language Models (LLMs) are a transformative technology in artificial intelligence, powering applications like chatbots, text generation, and automated analysis. These models, built on deep learning architectures, excel at understanding and generating human-like text by learning patterns from vast datasets. At their core, LLMs are neural networks trained on billions of words from diverse sources, such as books, websites, and social media, enabling them to capture the nuances of language, from grammar to context.\n",
    "The foundation of LLMs lies in the transformer architecture, introduced in 2017 with the paper \"Attention is All You Need.\" Transformers use mechanisms like self-attention to process input text, allowing the model to weigh the importance of each word relative to others in a sentence. This enables LLMs to handle long-range dependencies, making them adept at tasks like translation, summarization, and question-answering. Models like BERT, GPT, and Llama have pushed the boundaries of what machines can achieve, with each iteration scaling up in size and capability.\n",
    "Training an LLM involves feeding it massive text corpora and optimizing billions of parameters using techniques like supervised learning and reinforcement learning. The process is computationally intensive, requiring powerful GPUs or TPUs and significant energy resources. Once trained, LLMs can perform zero-shot or few-shot learning, meaning they can tackle tasks with little to no task-specific training, relying on their prelearned knowledge. For example, an LLM can generate a story from a prompt like \"Once upon a time\" or classify sentiment in a sentence without explicit retraining.\n",
    "LLMs are accessible through platforms like the Hugging Face Hub, which hosts pretrained models, datasets, and tools like the transformers library. This democratizes AI, allowing developers to use models like GPT-2 or DistilBERT for tasks such as text classification or generation without building from scratch. However, using LLMs responsibly requires understanding their limitations, including biases in training data, high computational costs, and potential security risks when loading untrusted models.\n",
    "Applications of LLMs span industries: they power virtual assistants, automate customer support, assist in coding, and enhance research by summarizing complex texts. Yet, challenges remain, including ensuring fairness, reducing environmental impact, and managing the risk of generating misleading information. As LLMs evolve, they promise to reshape how we interact with technology, making it critical to approach their development and use with care.\n",
    "In summary, LLMs represent a leap forward in AI, driven by transformers and massive datasets. Their ability to process and generate language has broad implications, but careful management is essential to harness their potential effectively..''')\n",
    "        print(s)\n",
    "    else:\n",
    "        print('HF_TOKEN not set; skipping hosted inference.')\n",
    "except Exception as e:\n",
    "    print('InferenceClient not available or error:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10869928",
   "metadata": {},
   "source": [
    "## Exercise — Model comparison and benchmarking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a52d5f",
   "metadata": {},
   "source": [
    "###  Task Description\n",
    "Create a comprehensive comparison of two sentiment classifiers using the IMDB dataset. Your analysis should include:\n",
    "\n",
    "#### Model Selection\n",
    "- Compare `distilbert-base-uncased-finetuned-sst-2-english` and `textattack/bert-base-uncased-SST-2`\n",
    "- Document the model architectures and sizes\n",
    "\n",
    "#### Implementation Requirements\n",
    "- Use the Hugging Face datasets library to load IMDB data\n",
    "- Implement batch processing for memory efficiency\n",
    "- Include proper error handling and device management\n",
    "- Record and compare inference times\n",
    "\n",
    "#### Evaluation Metrics\n",
    "- Calculate and compare accuracy scores\n",
    "- Generate classification reports\n",
    "- Record processing time per batch\n",
    "- Document memory usage where applicable\n",
    "\n",
    "#### Technical Requirements\n",
    "- Pin model revisions for reproducibility \n",
    "- Record label mappings from `config.id2label`\n",
    "- Use appropriate batch sizes (suggest starting with 32)\n",
    "\n",
    "#### Deliverables\n",
    "- Working code implementation\n",
    "- Performance comparison table\n",
    "- Brief analysis of tradeoffs (speed vs. accuracy)\n",
    "- Documentation of label mappings and any data preprocessing\n",
    "\n",
    "#### Bonus Tasks\n",
    "- Experiment with different dataset sizes\n",
    "- Add visualization of results\n",
    "- Compare memory usage across models\n",
    "- Analyze misclassified examples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
